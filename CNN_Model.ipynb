{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.8.1-cp38-cp38-win_amd64.whl (190.5 MB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'D:\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataLoaded:\n",
      "(1280, 1024, 60)\n"
     ]
    }
   ],
   "source": [
    "frameNum = 60 \n",
    "\n",
    "# load data\n",
    "dfs = []\n",
    "for i in range(1,33):\n",
    "  for j in range(1,41):\n",
    "    filename = f\"./Files/User{i}_Video{j}.dat\"\n",
    "    cols = np.arange(frameNum)\n",
    "    df = pd.read_csv(filename, header = None, usecols = cols, delimiter=' ')   \n",
    "    dfs.append(df.values)\n",
    "    #print('participant%dvideo%d.txt'%(i,j))\n",
    "\n",
    "dfs = np.array(dfs)\n",
    "print('dataLoaded:')\n",
    "print(dfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.744679e-05</td>\n",
       "      <td>2.553555e-04</td>\n",
       "      <td>6.081591e-04</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>1.358385e-03</td>\n",
       "      <td>1.512967e-03</td>\n",
       "      <td>1.435924e-03</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>6.851798e-04</td>\n",
       "      <td>2.540377e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.440838e-04</td>\n",
       "      <td>2.968252e-04</td>\n",
       "      <td>3.327871e-04</td>\n",
       "      <td>1.907146e-04</td>\n",
       "      <td>5.777137e-05</td>\n",
       "      <td>3.531563e-07</td>\n",
       "      <td>1.131937e-04</td>\n",
       "      <td>3.096933e-04</td>\n",
       "      <td>4.225250e-04</td>\n",
       "      <td>4.598667e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.896244e-04</td>\n",
       "      <td>5.140935e-04</td>\n",
       "      <td>9.558036e-04</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>1.749479e-03</td>\n",
       "      <td>1.818524e-03</td>\n",
       "      <td>1.688201e-03</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>7.126611e-04</td>\n",
       "      <td>2.659181e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.900513e-05</td>\n",
       "      <td>6.972865e-05</td>\n",
       "      <td>8.060219e-05</td>\n",
       "      <td>5.384183e-05</td>\n",
       "      <td>1.027467e-05</td>\n",
       "      <td>8.422783e-06</td>\n",
       "      <td>8.241706e-05</td>\n",
       "      <td>1.739351e-04</td>\n",
       "      <td>2.199536e-04</td>\n",
       "      <td>2.423155e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.424911e-04</td>\n",
       "      <td>8.062397e-04</td>\n",
       "      <td>1.327554e-03</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>1.963859e-03</td>\n",
       "      <td>1.990143e-03</td>\n",
       "      <td>1.746486e-03</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>7.093119e-04</td>\n",
       "      <td>2.072976e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.325482e-06</td>\n",
       "      <td>8.649716e-06</td>\n",
       "      <td>1.195799e-05</td>\n",
       "      <td>2.304283e-06</td>\n",
       "      <td>5.011289e-06</td>\n",
       "      <td>3.122549e-05</td>\n",
       "      <td>7.142586e-05</td>\n",
       "      <td>1.206939e-04</td>\n",
       "      <td>1.962808e-04</td>\n",
       "      <td>2.646049e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.990651e-04</td>\n",
       "      <td>1.194994e-03</td>\n",
       "      <td>1.604066e-03</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>2.080131e-03</td>\n",
       "      <td>1.977483e-03</td>\n",
       "      <td>1.597441e-03</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>5.215706e-04</td>\n",
       "      <td>1.201247e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236240e-05</td>\n",
       "      <td>3.730674e-06</td>\n",
       "      <td>4.530567e-07</td>\n",
       "      <td>8.414927e-06</td>\n",
       "      <td>2.729891e-05</td>\n",
       "      <td>6.257979e-05</td>\n",
       "      <td>1.118084e-04</td>\n",
       "      <td>1.828313e-04</td>\n",
       "      <td>2.560754e-04</td>\n",
       "      <td>2.950363e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.715428e-04</td>\n",
       "      <td>1.380071e-03</td>\n",
       "      <td>1.751842e-03</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>1.949584e-03</td>\n",
       "      <td>1.673408e-03</td>\n",
       "      <td>1.313303e-03</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>3.323800e-04</td>\n",
       "      <td>3.403212e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.602134e-05</td>\n",
       "      <td>2.763704e-06</td>\n",
       "      <td>2.016952e-06</td>\n",
       "      <td>3.032129e-05</td>\n",
       "      <td>6.418142e-05</td>\n",
       "      <td>1.023652e-04</td>\n",
       "      <td>1.876355e-04</td>\n",
       "      <td>2.840458e-04</td>\n",
       "      <td>3.488021e-04</td>\n",
       "      <td>4.249631e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>3.213220e-06</td>\n",
       "      <td>1.728500e-06</td>\n",
       "      <td>1.079931e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>7.633227e-07</td>\n",
       "      <td>1.424721e-06</td>\n",
       "      <td>2.315452e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.238352e-06</td>\n",
       "      <td>1.640884e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.683036e-07</td>\n",
       "      <td>9.850071e-07</td>\n",
       "      <td>3.738347e-07</td>\n",
       "      <td>1.770216e-07</td>\n",
       "      <td>2.586192e-07</td>\n",
       "      <td>2.687064e-07</td>\n",
       "      <td>4.844050e-07</td>\n",
       "      <td>9.182721e-07</td>\n",
       "      <td>2.673959e-07</td>\n",
       "      <td>7.516916e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>2.324575e-07</td>\n",
       "      <td>1.569091e-06</td>\n",
       "      <td>3.535182e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.087443e-07</td>\n",
       "      <td>4.513868e-07</td>\n",
       "      <td>2.613192e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>6.983612e-07</td>\n",
       "      <td>1.214008e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>3.827605e-08</td>\n",
       "      <td>1.169237e-07</td>\n",
       "      <td>1.233719e-07</td>\n",
       "      <td>4.624929e-08</td>\n",
       "      <td>1.528097e-07</td>\n",
       "      <td>6.242575e-07</td>\n",
       "      <td>2.100241e-07</td>\n",
       "      <td>3.001984e-08</td>\n",
       "      <td>6.512785e-08</td>\n",
       "      <td>5.664144e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>3.131241e-06</td>\n",
       "      <td>1.464914e-06</td>\n",
       "      <td>2.107083e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.875283e-06</td>\n",
       "      <td>6.668432e-07</td>\n",
       "      <td>1.704003e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.193310e-07</td>\n",
       "      <td>8.427626e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.094986e-07</td>\n",
       "      <td>1.536073e-08</td>\n",
       "      <td>3.565909e-07</td>\n",
       "      <td>3.117275e-08</td>\n",
       "      <td>2.581929e-07</td>\n",
       "      <td>1.334709e-08</td>\n",
       "      <td>1.250931e-08</td>\n",
       "      <td>7.376728e-09</td>\n",
       "      <td>1.594658e-07</td>\n",
       "      <td>8.691596e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1.391200e-06</td>\n",
       "      <td>2.521965e-07</td>\n",
       "      <td>9.026174e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.429012e-06</td>\n",
       "      <td>2.175078e-07</td>\n",
       "      <td>7.982079e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.299245e-06</td>\n",
       "      <td>9.996390e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.519980e-09</td>\n",
       "      <td>5.100703e-08</td>\n",
       "      <td>9.030283e-07</td>\n",
       "      <td>3.390141e-07</td>\n",
       "      <td>3.083695e-07</td>\n",
       "      <td>1.549144e-07</td>\n",
       "      <td>2.946007e-07</td>\n",
       "      <td>3.212838e-07</td>\n",
       "      <td>4.275782e-09</td>\n",
       "      <td>8.383055e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>6.839510e-07</td>\n",
       "      <td>8.108994e-07</td>\n",
       "      <td>2.925475e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.925222e-07</td>\n",
       "      <td>1.200958e-06</td>\n",
       "      <td>1.401392e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.074355e-06</td>\n",
       "      <td>3.365254e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.249694e-10</td>\n",
       "      <td>2.456957e-07</td>\n",
       "      <td>2.409760e-07</td>\n",
       "      <td>1.213905e-09</td>\n",
       "      <td>5.811238e-07</td>\n",
       "      <td>3.546804e-07</td>\n",
       "      <td>1.141992e-07</td>\n",
       "      <td>1.596708e-08</td>\n",
       "      <td>8.678191e-08</td>\n",
       "      <td>1.451196e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2         3             4   \\\n",
       "0     5.744679e-05  2.553555e-04  6.081591e-04  0.001022  1.358385e-03   \n",
       "1     1.896244e-04  5.140935e-04  9.558036e-04  0.001427  1.749479e-03   \n",
       "2     4.424911e-04  8.062397e-04  1.327554e-03  0.001792  1.963859e-03   \n",
       "3     6.990651e-04  1.194994e-03  1.604066e-03  0.001909  2.080131e-03   \n",
       "4     9.715428e-04  1.380071e-03  1.751842e-03  0.001996  1.949584e-03   \n",
       "...            ...           ...           ...       ...           ...   \n",
       "1019  3.213220e-06  1.728500e-06  1.079931e-06  0.000001  7.633227e-07   \n",
       "1020  2.324575e-07  1.569091e-06  3.535182e-06  0.000002  6.087443e-07   \n",
       "1021  3.131241e-06  1.464914e-06  2.107083e-07  0.000001  1.875283e-06   \n",
       "1022  1.391200e-06  2.521965e-07  9.026174e-07  0.000003  1.429012e-06   \n",
       "1023  6.839510e-07  8.108994e-07  2.925475e-06  0.000002  4.925222e-07   \n",
       "\n",
       "                5             6         7             8             9   ...  \\\n",
       "0     1.512967e-03  1.435924e-03  0.001143  6.851798e-04  2.540377e-04  ...   \n",
       "1     1.818524e-03  1.688201e-03  0.001282  7.126611e-04  2.659181e-04  ...   \n",
       "2     1.990143e-03  1.746486e-03  0.001241  7.093119e-04  2.072976e-04  ...   \n",
       "3     1.977483e-03  1.597441e-03  0.001060  5.215706e-04  1.201247e-04  ...   \n",
       "4     1.673408e-03  1.313303e-03  0.000843  3.323800e-04  3.403212e-05  ...   \n",
       "...            ...           ...       ...           ...           ...  ...   \n",
       "1019  1.424721e-06  2.315452e-06  0.000001  1.238352e-06  1.640884e-06  ...   \n",
       "1020  4.513868e-07  2.613192e-06  0.000004  6.983612e-07  1.214008e-09  ...   \n",
       "1021  6.668432e-07  1.704003e-06  0.000002  4.193310e-07  8.427626e-07  ...   \n",
       "1022  2.175078e-07  7.982079e-07  0.000004  2.299245e-06  9.996390e-09  ...   \n",
       "1023  1.200958e-06  1.401392e-06  0.000001  1.074355e-06  3.365254e-07  ...   \n",
       "\n",
       "                50            51            52            53            54  \\\n",
       "0     1.440838e-04  2.968252e-04  3.327871e-04  1.907146e-04  5.777137e-05   \n",
       "1     3.900513e-05  6.972865e-05  8.060219e-05  5.384183e-05  1.027467e-05   \n",
       "2     9.325482e-06  8.649716e-06  1.195799e-05  2.304283e-06  5.011289e-06   \n",
       "3     1.236240e-05  3.730674e-06  4.530567e-07  8.414927e-06  2.729891e-05   \n",
       "4     1.602134e-05  2.763704e-06  2.016952e-06  3.032129e-05  6.418142e-05   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1019  7.683036e-07  9.850071e-07  3.738347e-07  1.770216e-07  2.586192e-07   \n",
       "1020  3.827605e-08  1.169237e-07  1.233719e-07  4.624929e-08  1.528097e-07   \n",
       "1021  1.094986e-07  1.536073e-08  3.565909e-07  3.117275e-08  2.581929e-07   \n",
       "1022  1.519980e-09  5.100703e-08  9.030283e-07  3.390141e-07  3.083695e-07   \n",
       "1023  1.249694e-10  2.456957e-07  2.409760e-07  1.213905e-09  5.811238e-07   \n",
       "\n",
       "                55            56            57            58            59  \n",
       "0     3.531563e-07  1.131937e-04  3.096933e-04  4.225250e-04  4.598667e-04  \n",
       "1     8.422783e-06  8.241706e-05  1.739351e-04  2.199536e-04  2.423155e-04  \n",
       "2     3.122549e-05  7.142586e-05  1.206939e-04  1.962808e-04  2.646049e-04  \n",
       "3     6.257979e-05  1.118084e-04  1.828313e-04  2.560754e-04  2.950363e-04  \n",
       "4     1.023652e-04  1.876355e-04  2.840458e-04  3.488021e-04  4.249631e-04  \n",
       "...            ...           ...           ...           ...           ...  \n",
       "1019  2.687064e-07  4.844050e-07  9.182721e-07  2.673959e-07  7.516916e-09  \n",
       "1020  6.242575e-07  2.100241e-07  3.001984e-08  6.512785e-08  5.664144e-07  \n",
       "1021  1.334709e-08  1.250931e-08  7.376728e-09  1.594658e-07  8.691596e-08  \n",
       "1022  1.549144e-07  2.946007e-07  3.212838e-07  4.275782e-09  8.383055e-08  \n",
       "1023  3.546804e-07  1.141992e-07  1.596708e-08  8.678191e-08  1.451196e-08  \n",
       "\n",
       "[1024 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1024, 60)\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "x_min = dfs.min(axis = (1,2),keepdims=True)\n",
    "x_max = dfs.max(axis = (1,2),keepdims=True)\n",
    "dfs_normal = (dfs-x_min)/(x_max-x_min)\n",
    "print(dfs_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "depth = 3\n",
    "# divide frames ,or 60s is too long for a single 3dinput\n",
    "reshape_dfs = np.split(dfs_normal, frameNum/depth, axis=2)\n",
    "reshape_dfs = np.array(reshape_dfs)\n",
    "reshape_dfs = np.reshape(reshape_dfs,[-1,1024,depth])\n",
    "print(reshape_dfs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 1) (1280, 1) (1280, 1) (1280, 1)\n"
     ]
    }
   ],
   "source": [
    "# load label\n",
    "label_dfv = pd.read_csv('./labels_class_0.csv',\n",
    "    names = ['valence'], header=None, delimiter=',' )\n",
    "\n",
    "label_dfa = pd.read_csv('./labels_class_1.csv',\n",
    "    names = ['arousal'], header=None, delimiter=',' )\n",
    "\n",
    "label_dfd = pd.read_csv('./labels_class_2.csv',\n",
    "    names = ['dominance'], header=None, delimiter=',' )\n",
    "\n",
    "label_dfl = pd.read_csv('./labels_class_3.csv',\n",
    "    names = ['liking'], header=None, delimiter=',' )\n",
    "\n",
    "#label_dfv\n",
    "print(label_dfv.shape, label_dfa.shape, label_dfd.shape, label_dfl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dfv.iloc[5,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dfva = pd.DataFrame(columns=['valencearousal'])\n",
    "lb = {'00':0, '01':1, '10':2, '11':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valencearousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     valencearousal\n",
       "0                 3\n",
       "1                 3\n",
       "2                 3\n",
       "3                 3\n",
       "4                 2\n",
       "...             ...\n",
       "1275              1\n",
       "1276              1\n",
       "1277              1\n",
       "1278              1\n",
       "1279              2\n",
       "\n",
       "[1280 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1280):\n",
    "    label_dfva = label_dfva.append({'valencearousal':lb[str(label_dfv.iloc[i,0]) + str(label_dfa.iloc[i,0])]},ignore_index = True)\n",
    "label_dfva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    531\n",
       "1    285\n",
       "2    277\n",
       "0    187\n",
       "Name: valencearousal, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dfva['valencearousal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600,) (25600,) (25600,) (25600,)\n"
     ]
    }
   ],
   "source": [
    "labelv = np.tile(label_dfv['valence'].astype(int).values,20)\n",
    "labela = np.tile(label_dfa['arousal'].astype(int).values,20)\n",
    "labeld = np.tile(label_dfd['dominance'].astype(int).values,20)\n",
    "labell = np.tile(label_dfl['liking'].astype(int).values,20)\n",
    "print(labelv.shape, labela.shape, labeld.shape, labell.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600,)\n"
     ]
    }
   ],
   "source": [
    "labelva = np.tile(label_dfva['valencearousal'].astype(int).values,20)\n",
    "print(labelva.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20480, 1024, 3]) torch.Size([20480]) torch.Size([5120, 1024, 3]) torch.Size([5120])\n"
     ]
    }
   ],
   "source": [
    "# divive train & test\n",
    "x_train, x_test, y_train, y_test = train_test_split(reshape_dfs, labelva, test_size=0.2)\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(x_train.shape,  y_train.shape,x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv11 = nn.Conv3d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv12 = nn.Conv3d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool1 = nn.MaxPool3d(kernel_size=2, padding=(0,0,1))\n",
    "    \n",
    "    self.conv21 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv22 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool2 = nn.MaxPool3d(kernel_size=2, padding=(0,0,1))\n",
    "    \n",
    "    self.conv31 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv32 = nn.Conv3d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool3 = nn.MaxPool3d(kernel_size=2, padding=0)\n",
    "    \n",
    "\n",
    "    self.fc_layer = nn.Linear(128*4*4*1, 2)\n",
    "    \n",
    "    self.dropout_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "  def forward(self, xb):\n",
    "    h1 = self.conv11(xb)\n",
    "    h1 = self.conv12(h1)\n",
    "    h1 = self.dropout_layer(h1)\n",
    "    h1 = self.pool1(h1)\n",
    "    h1 = F.relu(h1)\n",
    "\n",
    "    h2 = self.conv21(h1)\n",
    "    h2 = self.conv22(h2)\n",
    "    #h2 = self.dropout_layer(h2)\n",
    "    h2 = self.pool2(h2)\n",
    "    h2 = F.relu(h2) \n",
    "\n",
    "    h3 = self.conv31(h2)\n",
    "    h3 = self.conv32(h3)\n",
    "    #h3 = self.dropout_layer(h3)\n",
    "    h3 = self.pool3(h3)\n",
    "    h3 = F.relu(h3) \n",
    "    \n",
    "    \n",
    "    # flatten the output from conv layers before feeind it to FC layer\n",
    "    flatten = h3.view(-1, 128*4*4*1)\n",
    "    out = self.fc_layer(flatten)\n",
    "    #out = self.dropout_layer(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, epochs=35 , batch_size=32, lr=0.0001, weight_decay=0):\n",
    "    # data\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = F.cross_entropy\n",
    "\n",
    "    # optimizer\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # figure\n",
    "    train_a = list([])\n",
    "    test_a = list([])\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        tic = time.time()\n",
    "        acc_train = []\n",
    "        for xb, yb in train_data_loader:    \n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        acc_train.append(pred.detach().argmax(1).eq(yb).float().mean().cpu().numpy())\n",
    "        acc_train = np.mean(acc_train)\n",
    "        toc = time.time()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_pred = model(x_test.to(device))\n",
    "            acc = y_pred.argmax(1).eq(y_test.to(device)).float().mean().cpu().numpy()\n",
    "\n",
    "        train_a.append(acc_train)\n",
    "        test_a.append(acc)\n",
    "        print('Epoch %d : train_acc: %f, test_acc: %f, running time: %d'% (epoch, acc_train, acc, toc-tic))\n",
    "\n",
    "    # print accuracies\n",
    "    print('Maximum Training Accuracy: ',max(train_a))\n",
    "    print('Maximum Testing Accuracy: ',max(test_a))\n",
    "    \n",
    "    # draw an accuracy figure\n",
    "    plt.plot(train_a,'y.-.')\n",
    "    plt.plot(test_a,'.-.')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = cnn_classifier()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "train_model(model, x_train.view(-1, 1, 32, 32, 3), y_train, x_test.view(-1, 1, 32, 32, 3), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20480, 1024, 3]) torch.Size([20480]) torch.Size([5120, 1024, 3]) torch.Size([5120])\n"
     ]
    }
   ],
   "source": [
    "# divide train & test\n",
    "x_train, x_test, y_train, y_test = train_test_split(reshape_dfs, labela, test_size=0.2)\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "print(x_train.shape,  y_train.shape,x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 : 0.681590, train_acc: 0.750000, test_acc: 0.642969, running time: 64\n",
      "Loss at epoch 1 : 0.674492, train_acc: 0.750000, test_acc: 0.642969, running time: 66\n",
      "Loss at epoch 2 : 0.666143, train_acc: 0.750000, test_acc: 0.642969, running time: 66\n",
      "Loss at epoch 3 : 0.656142, train_acc: 0.750000, test_acc: 0.642969, running time: 66\n",
      "Loss at epoch 4 : 0.643261, train_acc: 0.750000, test_acc: 0.642969, running time: 67\n",
      "Loss at epoch 5 : 0.626970, train_acc: 0.750000, test_acc: 0.642969, running time: 66\n",
      "Loss at epoch 6 : 0.607250, train_acc: 0.750000, test_acc: 0.642969, running time: 68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b58abc9fafa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-3a3d544a7883>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, x_train, y_train, x_test, y_test, epochs, batch_size, lr, weight_decay)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-29c7c8c5c522>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xb)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv21\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#h2 = self.dropout_layer(h2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    518\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_triple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m                             self.dilation, self.groups)\n\u001b[1;32m--> 520\u001b[1;33m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    521\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, x_train.view(-1, 1, 32, 32, 3), y_train, x_test.view(-1, 1, 32, 32, 3), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)  \n",
    "a = torch.zeros(4,3)    \n",
    "a = a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
